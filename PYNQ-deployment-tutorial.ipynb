{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512f4db5",
   "metadata": {},
   "source": [
    "# fpgaConvNet deployment on PYNQ\n",
    "\n",
    "This notebook shows how to run [fpgaConvNet](https://alexmontgomerie.github.io/fpgaconvnet-website/) IPs under the PYNQ framwork using the fpgaConvNet PYNQ driver. The model used here is the same as the one used in the [fpgaConvNet tutorial](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/), whose IP can be generated with the [end-to-end example](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/end-to-end-example.ipynb) and whose bitstream can be generated by following the Parts 1 to 4 of the [hardware tutorial](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/hardware-tutorial.ipynb). Make sure to select the PYNQ-supported board/part during the fpgaConvNet and [SAMO](https://github.com/AlexMontgomerie/samo) workflow and at the creation of the Vivado project.\n",
    "\n",
    "To use load the model with the fpgaConvNet PYNQ driver, the following files are required:\n",
    "- The bitstream file (in this example, [`single_layer_tutorial.bit`](single_layer_tutorial.bit))\n",
    "- The hardware handoff file (in this example, [`single_layer_tutorial.hwh`](single_layer_tutorial.hwh))\n",
    "- The `.json` file describing the partitions of the fpgaConvNet model (in this example, [`single_layer_tutorial.json`](single_layer_tutorial.json))\n",
    "\n",
    "**The files (`.bit`, `.hwh` and `.json`) in this tutorial have been generated for the PYNQ-Z2 board.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70606058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq import Overlay\n",
    "from assets.tutorial_library import *\n",
    "import numpy as np\n",
    "from fpgaconvnet_pynq_driver import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5449ff",
   "metadata": {},
   "source": [
    "## Get MNIST dataset\n",
    "\n",
    "This will download the MNIST dataset and save it in the `MNIST` folder if it is not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893d6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('assets/MNIST/t10k-images-idx3-ubyte'):\n",
    "    os.system('cd assets/MNIST && ./get_mnist.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6e863",
   "metadata": {},
   "source": [
    "## Load the fpgaConvNet overlay\n",
    "In order to load an overlay IP on PYNQ you need to upload to the platform the `.bit` (bitstream) and `.hwh` files, which can be found inside the `.xsa` file generated by Vivado during the hardware exportation. Also, to the fpgaConvNet PYNQ driver needs the `.json` file with the description of the implemented partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f949f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overlay = Overlay('single_layer_tutorial.bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpgaconvnet_ip = overlay.fpgaconvnet_ip_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00832e",
   "metadata": {},
   "source": [
    "The fpgaConvNet-generated IPs are automatically linked to the `FpgaConvnetDriver` defined at `fpgaconvnet_pynq_driver.py`. The `load_partition` obtains the dimensionally of the partition and allocates the input and output buffers, setting the model to be ready to start inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc34a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpgaconvnet_ip.load_partition('single_layer_tutorial.json', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cf6fb",
   "metadata": {},
   "source": [
    "## Test the implementation\n",
    "\n",
    "Load the example input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c3a2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_MNIST_image(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cdbcd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19831660",
   "metadata": {},
   "source": [
    "> The fpgaConvNet driver expects the input datatype to have 4 dims: batch size, channels, rows and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bae39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.expand_dims(img, 0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e8da5",
   "metadata": {},
   "source": [
    "**The entire inference functionality of the fpgaConvNet IP under PYNQ is embedded in the `.run()` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3d31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = fpgaconvnet_ip.run(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf034e2b",
   "metadata": {},
   "source": [
    "Now, let's check if the model is working as expected. This cell loads the the outputs of sample `0` of the ONNX model used as staring point at the [end-to-end example](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/end-to-end-example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2b2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_onnx = np.load('assets/output.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fdbf8a",
   "metadata": {},
   "source": [
    "Compute the _Mean Squared Error_ (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affce805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3678097e-05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((Y.flatten()-Y_onnx)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ec262",
   "metadata": {},
   "source": [
    "## Performance testing\n",
    "\n",
    "Finally, the driver also provides a function to measure the performance of the model in terms of latency and throughput. If the platforms supports power measurement through the PMBus (as in the ZCU104 board), the power consumption is also measured, and the energy per inference is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee0ea6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting latency...Done\n",
      "Latency: 2.42 ms\n",
      "Throughput: 413.45 inferences/s\n"
     ]
    }
   ],
   "source": [
    "fpgaconvnet_ip.test_performance(img, int(1e4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
